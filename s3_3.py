from mxnet import autograd, nd

# è®­ç»ƒæ•°æ®ä¸ºé•¿åº¦2çš„å‘é‡
num_inputs = 2

# è®­ç»ƒæ•°æ®1000ä¸ª
num_examples = 1000

# è®¾ç½®åˆå§‹æƒé‡
true_w = [2, -3.4]

# è®¾ç½®bçš„å€¼
true_b = 4.2

# ç”ŸæˆX
# 1000 x 2
# å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
features = nd.random.normal(scale = 1, shape = (num_examples, num_inputs))

# ç”Ÿæˆy
# 1000
# y = w1 * ğ‘¿1 + w2 * X2 + ğ‘
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b

# Ïµ ~ Epsilon
# åŠ ä¸Šå™ªå£°
epsilon = nd.random.normal(scale = 0.01, shape = labels.shape)

labels += epsilon

# Gluonæä¾›äº†dataåŒ…æ¥è¯»å–æ•°æ®
from mxnet.gluon import data as gdata

# æ‰¹æ¬¡å¤§å°ä¸º10
batch_size = 10

# æŠŠfeaturesï¼ˆXï¼‰å’Œlabelsï¼ˆyï¼‰æ”¾å…¥datasetä¸­
dataset = gdata.ArrayDataset(features, labels)

# dataset
# batch_size 10
data_iter = gdata.DataLoader(dataset, batch_size, shuffle = True)

# æ‰“å°ä¸€æ‰¹æ¥çœ‹ä¸‹ï¼Œæ‰“å°Xï¼Œy
for X, y in data_iter:
    # X     10 x 2
    # y     10
    # print(X, y)
    break

# å¯¼å…¥nnæ¨¡å—ï¼Œ(nn  ---  neural networksï¼ˆç¥ç»ç½‘ç»œï¼‰çš„ç¼©å†™)
# è¯¥æ¨¡å—å®šä¹‰äº†å¤§é‡ç¥ç»ç½‘ç»œçš„å±‚
from mxnet.gluon import nn

# sequential    çº¿æ€§åºåˆ—
# è·å–ä¸€ä¸ªè®­ç»ƒçš„æ¨¡å‹
# ğ’š = ğ‘¿ğ’˜ + ğ‘
net = nn.Sequential()

# å•å±‚ç¥ç»ç½‘ç»œï¼Œå±‚æ•°ä¸º1
net.add(nn.Dense(1))

# å¯¼å…¥initæ¨¡å—
from mxnet import init

# ç”Ÿæˆæ¨¡å‹wåˆå§‹å€¼ï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒ
net.initialize(init.Normal(sigma=0.01))

# å¯¼å…¥lossæ¨¡å—ï¼Œåˆ«åä¸ºglossï¼Œä½¿ç”¨ä»–çš„å¹³æ–¹æŸå¤±ä½œä¸ºæ¨¡å‹losså‡½æ•°
from mxnet.gluon import loss as gloss

# å¹³æ–¹æŸå¤±åˆç§°L2èŒƒæ•°æŸå¤±
# å‚è€ƒåé¢çš„L2èŒƒæ•°
loss = gloss.L2Loss()

from mxnet import gluon

# å®šä¹‰ä¸€ä¸ªè®­ç»ƒåŠ©ç†
# sgdï¼ˆæ¢¯åº¦ä¸‹é™)ï¼Œå­¦ä¹ ç‡ä¸º0.03
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})

# è®­ç»ƒä¸‰è½®
num_epochs = 3
for epoch in range(1, num_epochs + 1):
    for X, y in data_iter:
        with autograd.record():
            l = loss(net(X), y)

        # è®¡ç®—æ¢¯åº¦
        l.backward()

        # å­¦ä¹ ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
        trainer.step(batch_size)

    # ç”¨å½“å‰çš„æ¨¡å‹æŸ¥çœ‹æŸå¤±
    l = loss(net(features), labels)
    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))

# å¯¹æ¯”çœŸå®çš„wå’Œæ¨¡å‹è®­ç»ƒå‡ºæ¥çš„w
print(true_w, net[0].weight.data())

# å¯¹æ¯”çœŸå®çš„bå’Œæ¨¡å‹è®­ç»ƒå‡ºæ¥çš„b
print(true_b, net[0].bias.data())