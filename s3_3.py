from mxnet import autograd, nd

# è®­ç»ƒæ•°æ®ä¸ºé•¿åº¦2çš„å‘é‡
num_inputs = 2

# è®­ç»ƒæ•°æ®1000ä¸ª
num_examples = 1000

# è®¾ç½®åˆå§‹æƒé‡
true_w = [2, -3.4]

# è®¾ç½®bçš„å€¼
true_b = 4.2

# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„çŸ©é˜µ (1000, 2)
# 1000 x 2
# ---è®­ç»ƒç‰¹å¾æ•°æ®---
#
# ------è®­ç»ƒå‡ºæ¥çš„å¸¦æœ‰ç‰¹å¾çš„æ•°æ®------
features = nd.random.normal(scale = 1, shape = (num_examples, num_inputs))

# æ±‚ğ‘¿ğ’˜+ğ‘      1000 * 2  2 * 1  b
# [2] * (1000) + [-3.4] * (1000)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b

# åŠ ä¸Šå™ªå£°
labels += nd.random.normal(scale = 0.01, shape = labels.shape)

from mxnet.gluon import data as gdata

# Gluonæä¾›äº†dataåŒ…æ¥è¯»å–æ•°æ®

# æ‰¹æ¬¡å¤§å°ä¸º10
batch_size = 10

# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ(çº¿æ€§å›å½’ä¹ˆï¼Œé¦–å…ˆæŠŠxï¼Œyçš„å€¼è®¾ç½®è¿›å»)ï¼Œç”Ÿæˆä¸€ä¸ªdatasetå¯¹è±¡
dataset = gdata.ArrayDataset(features, labels)

# éšæœºè¯»å–å°æ‰¹é‡ï¼Œä¼ å…¥æ•°æ®å¯¹è±¡ï¼Œä¼ å…¥æ‰¹æ¬¡
# å‚è€ƒä¹‹å‰çš„ä»£ç ï¼Œä¹‹å‰ä¹Ÿæœ‰è¿™ä¸ªå‡½æ•°ï¼Œåªä¸è¿‡æ˜¯æˆ‘è‡ªå·±å†™çš„
data_iter = gdata.DataLoader(dataset, batch_size, shuffle = True)

# æ‰“å°ä¸€æ‰¹æ¥çœ‹ä¸‹ï¼Œæ‰“å°Xï¼Œy
for X, y in data_iter:
    print(X, y)
    break

# å¯¼å…¥nnæ¨¡å—ï¼ŒnnæŒ‡çš„æ˜¯neural networksï¼ˆç¥ç»ç½‘ç»œï¼‰çš„ç¼©å†™
# è¯¥æ¨¡å—å®šä¹‰äº†å¤§é‡ç¥ç»ç½‘ç»œçš„å±‚
from mxnet.gluon import nn

# å®šä¹‰ä¸€ä¸ªæ¨¡å‹å˜é‡net
# ä¹‹å‰çš„netå®é™…ä¸Šæ˜¯ç”¨æ¥åšè®¡ç®—çš„
# ğ’š=ğ‘¿ğ’˜+ğ‘
net = nn.Sequential()

# å•å±‚ç¥ç»ç½‘ç»œï¼Œå±‚æ•°ä¸º1
net.add(nn.Dense(1))

# å¯¼å…¥initæ¨¡å—
from mxnet import init

# init.Normal(sigma=0.01) å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒ
net.initialize(init.Normal(sigma=0.01))

# å¯¼å…¥lossæ¨¡å—ï¼Œåˆ«åä¸ºglossï¼Œä½¿ç”¨ä»–çš„å¹³æ–¹æŸå¤±ä½œä¸ºæ¨¡å‹losså‡½æ•°
from mxnet.gluon import loss as gloss

# å¹³æ–¹æŸå¤±åˆç§°L2èŒƒæ•°æŸå¤±
# ä¹‹å‰çš„lossæ˜¯è‡ªå·±å†™çš„
loss = gloss.L2Loss()

from mxnet import gluon

# å®šä¹‰ä¸€ä¸ªè®­ç»ƒåŠ©ç†
# å½“å‰ç”¨sgdï¼Œæ¢¯åº¦ä¸‹é™ï¼Œå­¦ä¹ ç‡ä¸º0.03
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03})

# --------------------------
# å¼€å§‹è®­ç»ƒæ¨¡å‹


# æ¨¡æ‹Ÿä¹‹å‰çš„è®­ç»ƒä¸‰æ¬¡
num_epochs = 3
for epoch in range(1, num_epochs + 1):
    for X, y in data_iter:
        with autograd.record():
            # å¯ä»¥å’Œä¹‹å‰çš„ä»£ç å¯¹æ¯”ï¼Œä¹‹å‰çš„lossæ˜¯è‡ªå·±å†™çš„
            l = loss(net(X), y)
        l.backward()

        # è®­ç»ƒå™¨å¼€å§‹stepä¸€æ¬¡çš„æ¢¯åº¦
        trainer.step(batch_size)

    l = loss(net(features), labels)
    print('epoch %d, loss: %f' % (epoch, l.mean().asnumpy()))

dense = net[0]
true_w, dense.weight.data()

true_b, dense.bias.data()