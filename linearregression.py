from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd, nd
import random

######################################################
# ç”Ÿæˆæµ‹è¯•æ•°æ®
######################################################

# ğ’š=ğ‘¿ğ’˜+ğ‘+ğœ–

# è®­ç»ƒæ•°æ®ä¸ºé•¿åº¦2çš„å‘é‡
num_inputs = 2

# è®­ç»ƒæ•°æ®1000ä¸ª
num_examples = 1000

# è®¾ç½®åˆå§‹æƒé‡
true_w = [2, -3.4]

# è®¾ç½®bçš„å€¼
true_b = 4.2

# ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„çŸ©é˜µ (1000, 2)
features = nd.random.normal(scale = 1, shape = (num_examples, num_inputs))

# features[:, 0]æŠŠçŸ©é˜µè£å‰ªæˆå‘é‡
# features[:, 1]æŠŠçŸ©é˜µè£å‰ªæˆå‘é‡
# æ±‚ğ‘¿ğ’˜+ğ‘      1000 * 2  2 * 1  b
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b

# åŠ ä¸Šå™ªå£°ï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01ï¼Œæœ€ç»ˆæ±‚å‡ºyï¼Œæ³¨æ„æ­¤å¤„å™ªå£°æ˜¯æœä»æ­£æ€åˆ†å¸ƒçš„å‘é‡
# labels += nd.random.normal(scale = 0.01, shape = labels.shape)

# æ‰“å°è®­ç»ƒæ•°æ®å’Œyçš„ç¬¬ä¸€ä¸ªå€¼
print('-' * 20, "æ‰“å°è®­ç»ƒæ•°æ®å’Œyçš„ç¬¬ä¸€ä¸ªå€¼")
print(features[0], labels[0])

# è®¾ç½®æ‰“å°æ•°æ®æ“ä½œ
def use_svg_display():
    # ç”¨çŸ¢é‡å›¾æ˜¾ç¤º
    display.set_matplotlib_formats('svg')

def set_figsize(figsize=(3.5, 2.5)):
    use_svg_display()
    # è®¾ç½®å›¾çš„å°ºå¯¸
    plt.rcParams['figure.figsize'] = figsize

set_figsize()

# æ‰“å°è®­ç»ƒæ•°æ®
# features[:, 1].asnumpy()å¯ä»¥æŠŠNDArrayå¯¹è±¡è½¬æ¢æˆæ•°ç»„
# ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯xè½´çš„å‘é‡ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯yè½´çš„å‘é‡
# xæ˜¯è®­ç»ƒæ•°æ®çš„å€¼ï¼Œyæ˜¯è¾“å‡ºç»“æœçš„å€¼ï¼Œç„¶åå¯ä»¥æ ¹æ®å…¬å¼æ±‚å‡ºçº¿æ€§å›å½’ï¼Œä¹Ÿå¯ä»¥æ ¹æ®æ¢¯åº¦ç®—å‡ºçº¿æ€§å›å½’
plt.scatter(features[:, 1].asnumpy(), labels.asnumpy(), 1)

# æ‰“å°è®­ç»ƒæ•°æ®
# plt.show()

######################################################
# è¯»å–æ•°æ®é›†åˆ
######################################################

def data_iter(batch_size, features, labels):
    # è·å–è®­ç»ƒæ•°æ®çš„é•¿åº¦ï¼Œå½“å‰é•¿åº¦ä¸º1000
    num_examples = len(features)

    # æ ¹æ®é•¿åº¦ç”Ÿæˆä¸€ä¸ªä¸‹æ ‡indexçš„æ•°ç»„ï¼Œæ•°ç»„é•¿åº¦ä¸º1000
    indices = list(range(num_examples))

    # æŠŠæ•°ç»„çš„é¡ºåºæ‰“ä¹±
    random.shuffle(indices)

    for i in range(0, num_examples, batch_size):
        # å½“å‰çš„batch_sizeä¸º10ï¼Œç›¸å½“äºæ¯ä¸€æ‰¹é€‰æ‹©10ä¸ª
        #
        # indices[i: min(i + batch_size, num_examples)]
        # é¦–å…ˆæ˜¯åˆ‡ç‰‡æ“ä½œå–å‡º10ä¸ª
        # ç„¶åè½¬æ¢æˆNDArrayå¯¹è±¡
        j = nd.array(indices[i: min(i + batch_size, num_examples)])

        # æ ¹æ®ï¼ˆæ•°ç»„ or å‘é‡ï¼‰jçš„ç´¢å¼•å–å‡ºå¯¹åº”çš„å€¼
        yield features.take(j), labels.take(j)

# å½“å‰è®­ç»ƒçš„æ‰¹æ¬¡ä¸º10ä¸ªä¸€æ‰¹
batch_size = 10

# for X, y in data_iter(batch_size, features, labels):
#     # æ‰“å°10ä¸ªä¸€æ‰¹çš„è®­ç»ƒæ•°æ®ï¼Ÿ
#     print(X, y)
#     break

######################################################
# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
######################################################

# éšæœºç”Ÿæˆç¬¦åˆæ­£æ€åˆ†å¸ƒçš„æƒé‡å‘é‡
w = nd.random.normal(scale = 0.01, shape = (num_inputs, 1))

# éšæœºç”Ÿæˆç¬¦åˆæ­£æ€åˆ†å¸ƒçš„åç½®æ•°å€¼
b = nd.zeros(shape = (1,))

# ç”Ÿæˆæ¢¯åº¦
w.attach_grad()

# é»˜è®¤çš„æ¢¯åº¦ä¸º0
print("w.grad:", w.grad)

# ç”Ÿæˆæ¢¯åº¦
b.attach_grad()

# é»˜è®¤æ¢¯åº¦ä¸º0
print("b.grad:", b.grad)

# è¾“å…¥è®­ç»ƒæ•°æ®ï¼Œç„¶åè¾“å‡ºç®—å‡ºçš„ç»“æœ
# X:å…¥å‚çŸ©é˜µ  å½“å‰ä¸º 10 x 2
# w:æƒé‡å‘é‡  å½“å‰ä¸º 2 x 1
# b:åç½®ï¼ˆæ­¤å¤„ç”¨äº†å¹¿æ’­ï¼‰
# ç»“æœæ˜¯ 10 x 1
def linreg(X, w, b):
    return nd.dot(X, w) + b

# è®¡ç®—losså‡½æ•°ï¼Œå®é™…ä¸Šæ˜¯è®¡ç®—è¯¯å·®C
# y_hat è®¡ç®—å‡ºæ¥çš„æ•°å€¼ 10 x 1
# y çœŸå®ç»“æœ
# è¿”å›çš„æŸå¤± 10 x 1
def squared_loss(y_hat, y):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2

# æ¢¯åº¦ä¸‹é™
# params = [w, b]
# lr å½“å‰ä¸º0.03
# batch_size å½“å‰ä¸º10
def sgd(params, lr, batch_size):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    for param in params:
        param[:] = param - lr * param.grad / batch_size

# æ¢¯åº¦ä¸‹é™ç‡
lr = 0.03

# ä¸€å…±æ¢¯åº¦ä¸‹é™3æ¬¡
num_epochs = 3

net = linreg

loss = squared_loss

for epoch in range(num_epochs):  # è®­ç»ƒæ¨¡å‹ä¸€å…±éœ€è¦num_epochsä¸ªè¿­ä»£å‘¨æœŸ
    # åœ¨æ¯ä¸€ä¸ªè¿­ä»£å‘¨æœŸä¸­ï¼Œä¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ä¸€æ¬¡ï¼ˆå‡è®¾æ ·æœ¬æ•°èƒ½å¤Ÿè¢«æ‰¹é‡å¤§å°æ•´é™¤ï¼‰ã€‚X
    # å’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
    for X, y in data_iter(batch_size, features, labels):
        # é¦–å…ˆä»æ ·æœ¬ä¸­é€‰å‡º10ä¸ªæ ·æœ¬

        with autograd.record():
            # æ±‚å‡ºæŸå¤±å‡½æ•°
            l = loss(net(X, w, b), y)
        # æ±‚æ¢¯åº¦ï¼Œæ±‚wå’Œbçš„å¯¼æ•°ï¼Œç„¶åæŠŠæ•°å€¼ä»£å…¥è¿›å»
        # å°æ‰¹é‡çš„æŸå¤±å¯¹æ¨¡å‹å‚æ•°æ±‚æ¢¯åº¦
        l.backward()

        # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°
        # [w, b]æ˜¯NDArrayæ•°ç»„ï¼Œä¸¤ä¸ªå‚æ•°éƒ½æ˜¯NDArrayå¯¹è±¡
        sgd([w, b], lr, batch_size)
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().asnumpy()))